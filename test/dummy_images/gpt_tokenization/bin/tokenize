#!/usr/bin/env python
# coding: utf-8

import sys
import codecs
from nltk.tokenize import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer()

# Manually split a few words
SPLITS = {
    "sentence": ["Ġsen", "tence"]
}

for line in codecs.open(sys.argv[1], encoding="utf-8"):
    ret = []
    for i, token in enumerate(tokenizer.tokenize(line.strip())):
        if token in SPLITS:
            ret.extend(SPLITS[token])
        elif i == 0:
            # gpt doesn't add identifier to start-of-sentence
            ret.append(token)
        else:
            ret.append("Ġ" + token)

    print(" ".join(ret))
