#!/usr/bin/env python

import sys
import codecs
from nltk.tokenize import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer()

# Manually split a few words
SPLITS = {
    "sentence": ["sen", "##tence"]
}

for line in codecs.open(sys.argv[1], encoding="utf-8"):
    ret = []
    for token in tokenizer.tokenize(line.strip()):
        if token in SPLITS:
            ret.extend(SPLITS[token])
        else:
            ret.append(token)

    print(" ".join(ret))
